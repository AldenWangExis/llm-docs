import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,f as i,d as n,r as l,o as h}from"./app-Cp5jA_bi.js";const p={};function o(d,s){const a=l("Mermaid");return h(),e("div",null,[s[0]||(s[0]=i('<h1 id="llm-概率推理引擎" tabindex="-1"><a class="header-anchor" href="#llm-概率推理引擎"><span>LLM：概率推理引擎</span></a></h1><h2 id="从确定性到概率性的跨越" tabindex="-1"><a class="header-anchor" href="#从确定性到概率性的跨越"><span>从确定性到概率性的跨越</span></a></h2><p>传统软件工程追求精确：相同的输入必然产生相同的输出。但现实世界的许多任务本质上是模糊的——用户意图含糊不清、业务规则存在灰色地带、边界条件难以穷举。如果用rule-based代码处理这些模糊场景，你会陷入无穷无尽的if-else分支，最终发现这条路根本不可行。</p><p>LLM的价值恰恰在此：它基于概率统计训练，天然能&quot;理解&quot;自然语言中的模糊表达。不需要你为每种可能性编写规则，它通过学习海量文本中的模式，直接处理不确定性。这不是技术路线的妥协，而是计算范式的切换。</p><h2 id="本质机制-下一个token的预测器" tabindex="-1"><a class="header-anchor" href="#本质机制-下一个token的预测器"><span>本质机制：下一个Token的预测器</span></a></h2><p>LLM的核心操作极其简洁：接收一个文本序列，输出下一个Token的概率分布。</p><p>所谓Token，并非完整单词，而是由分词器切分后的最小语义单元。英文单词<code>&quot;university&quot;</code>被拆解为<code>[&quot;uni&quot;, &quot;vers&quot;, &quot;ity&quot;]</code>三个Token，中文词汇<code>&quot;人工智能&quot;</code>可能被切分为<code>[&quot;人工&quot;, &quot;智能&quot;]</code>或<code>[&quot;人&quot;, &quot;工&quot;, &quot;智&quot;, &quot;能&quot;]</code>，具体取决于分词器的训练语料。模型接收这些Token序列后，在高维向量空间中计算各候选Token的条件概率，选择概率最高的输出。</p><p>这个&quot;高维向量空间&quot;并非玄学概念，而是一个可度量的几何结构。每个Token被映射为一个包含数百到数千维度的数值向量。关键特性在于：<strong>向量之间的距离反映了语义的相似度</strong>。<code>&quot;国王&quot;</code> 与 <code>&quot;皇帝&quot;</code> 的向量距离远小于 <code>&quot;国王&quot;</code> 与 <code>&quot;键盘&quot;</code> 的距离；更精妙的是，通过向量运算可以捕捉类比关系，如 <code>vector(&quot;国王&quot;) - vector(&quot;男人&quot;) + vector(&quot;女人&quot;)</code> 的结果会接近 <code>vector(&quot;女王&quot;)</code>。这种将离散符号转化为连续空间中可计算的数值表示，是模型能够&quot;理解&quot;语义、执行推理的数学基础。</p>',8)),n(a,{code:"eJxtkMFOg0AQhu8+xTyANICQEg9erF6aRk0TPRgPA53omC1blyWWt3eYobZNyol83///u9mfHjcB23j/hSFegXyRoyN4pSb6AOsdNgRr2kqEG1hwFwPXfWTfanif4J47WCC7AZ7qb2l1kCR38OwdSwOd/P1S0PBg4RXK/ph5lFlHqg7XSLKJntT+XX5wety5u7HZC6Ywc9xbcvt5C+/prCqvIZ3l5Yfih+2Ogg9mKjWVmZeeqDU+H/k8N75Co4XuZFP6zctjGbf0xJc01B7DRlWmlTKdhnzfkfF85IVU/gAmUnQC"}),s[1]||(s[1]=i(`<div class="hint-container info"><p class="hint-container-title">图表说明</p><p>向量空间中的语义分布（二维投影）：X轴表示&quot;日常物品-政治权力&quot;维度，Y轴表示&quot;男性-女性&quot;维度。可以看到&quot;国王&quot;和&quot;皇帝&quot;紧密聚集在右下象限，&quot;女王&quot;在右上象限，而&quot;键盘&quot;和&quot;鼠标&quot;远离政治权力簇。向量运算&quot;国王-男人+女人≈女王&quot;的几何意义即：沿Y轴向上移动。</p></div><p>这一机制决定了LLM的所有能力——代码生成、逻辑推理、文本翻译——本质上都是在万亿参数构成的概率空间中寻找最符合统计规律的路径。</p><div class="hint-container tip"><p class="hint-container-title">工程师类比</p><p>将LLM视为一个支持自然语言调用的API端点。输入是Prompt字符串，输出是概率采样后的文本序列。但与REST API不同，相同的请求可能返回不同的响应，因为采样过程引入了随机性。</p></div><h2 id="工程接口定义" tabindex="-1"><a class="header-anchor" href="#工程接口定义"><span>工程接口定义</span></a></h2><p>从API设计的角度，LLM可以抽象为如下函数签名：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> llm_predict</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">    prompt</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> str</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">    temperature</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> float</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.7</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">    max_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> int</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 2048</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) -&gt; </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">str</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    prompt: 输入的上下文序列</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    temperature: 采样随机性 [0.0-2.0]</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">                 0.0 = 确定性最大化，适合逻辑推理</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">                 1.0 = 平衡创造力与准确性</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">                 2.0 = 高度发散，适合创意生成</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    max_tokens: 生成序列的最大长度</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    pass</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="核心参数的工程含义" tabindex="-1"><a class="header-anchor" href="#核心参数的工程含义"><span>核心参数的工程含义</span></a></h3><p><strong>Context Window（上下文窗口）</strong></p><p>这是LLM的&quot;工作内存&quot;。早期GPT-4仅支持4K Token，现代模型已扩展至256K甚至更长。所有的历史对话、系统指令、参考资料都必须装入这个窗口。一旦超出限制，早期的上下文会被截断，模型陷入&quot;失忆&quot;。</p><p>从成本角度看，上下文窗口的使用遵循线性定价模型：Token数量直接决定API调用费用。优化上下文长度是生产环境的关键指标。</p><p><strong>Prompt Engineering（提示工程）</strong></p><p>这不是玄学，而是用自然语言编写指令集。Prompt的结构会直接影响模型在向量空间中的推理路径。</p><p>推荐的Prompt架构遵循三段式：</p><div class="language-markdown line-numbers-mode" data-highlighter="shiki" data-ext="markdown" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-markdown"><span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;"># System: 定义硬性约束和角色设定</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">你是一个资深后端架构师，精通分布式系统设计。</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">输出必须遵循JSON Schema规范，禁止虚构API。</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;"># Context: 提供背景信息和参考资料</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">项目当前使用 PostgreSQL + Redis，日均QPS 10万。</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;"># Task: 明确任务目标和输出格式</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">设计一套缓存预热策略，输出实现步骤和关键代码片段。</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这种结构的工程价值在于：先定义规则可以剪除不需要的推理分支；设定角色缩小模型的&quot;搜索空间&quot;；明确任务确保输出的可操作性。</p><p><strong>无状态性与会话管理</strong></p><p>LLM本身不存储任何历史状态。每次API调用都是独立的无状态事务。要实现多轮对话，需要手动维护消息列表，并在每次请求时全量传递：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">conversation_history </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;system&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;你是Python专家&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;如何实现单例模式？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;assistant&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;...&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;这种模式线程安全吗？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 新请求</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">response </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> llm_predict</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(conversation_history)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这种设计类似于HTTP的无状态特性，将状态管理的责任转移到客户端。</p><h2 id="推理能力的工程实现" tabindex="-1"><a class="header-anchor" href="#推理能力的工程实现"><span>推理能力的工程实现</span></a></h2><p>虽然LLM本质是统计模式匹配，但通过架构设计可以模拟出&quot;推理&quot;能力。</p><h3 id="chain-of-thought-思维链" tabindex="-1"><a class="header-anchor" href="#chain-of-thought-思维链"><span>Chain-of-Thought（思维链）</span></a></h3><p>强制模型输出中间步骤，将隐式推理显式化。对比：</p><p><strong>直接推理（易出错）</strong>：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>Q: 咖啡店有23杯咖啡，卖出17杯，又制作了8杯，现在有多少杯？</span></span>
<span class="line"><span>A: 14杯</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>思维链推理（准确率提升）</strong>：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>Q: 咖啡店有23杯咖啡，卖出17杯，又制作了8杯，现在有多少杯？</span></span>
<span class="line"><span>A: 让我们逐步计算：</span></span>
<span class="line"><span>   1. 初始库存：23杯</span></span>
<span class="line"><span>   2. 卖出后：23 - 17 = 6杯</span></span>
<span class="line"><span>   3. 制作后：6 + 8 = 14杯</span></span>
<span class="line"><span>   答案：14杯</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在Prompt中添加<code>&quot;让我们逐步思考&quot;</code>（Let&#39;s think step by step）即可激活这一模式。原理是强制模型在输出最终答案前先生成推理路径Token，这些中间Token会影响后续概率分布，从而提高准确性。</p><h3 id="tree-of-thought-思维树" tabindex="-1"><a class="header-anchor" href="#tree-of-thought-思维树"><span>Tree-of-Thought（思维树）</span></a></h3><p>将线性推理扩展为树状探索。模型在每个决策点生成多个候选路径，评估中间状态，必要时回溯。适用于数独求解、代码调试等需要试错的场景。</p>`,30)),n(a,{code:"eJxLL0osyFAIceFSAIKg/PyS6JfT171cNOPF1u3Pm3bGKujq2ik4Rj+btvPZwg7HWLgqsLgTVNwJTdwZKu4MEXeEGGIY7fh8VsvTtROerV36ctUSQxRJI1RJIyuFF+tbnuzZ8HTJxhdblkKUOkEsNYx2QjEHpvRZx4SnXfOhSg3BaoPzc0pLMvPzop/NaXi+uwPiKogKMFFcUpmTCrRcIS0zJ8dKOS0tGQiQZIDGQGSSk0FySDIwk6HyZmbJyWZmXABinoPt"}),s[2]||(s[2]=i('<h2 id="能力边界与工程约束" tabindex="-1"><a class="header-anchor" href="#能力边界与工程约束"><span>能力边界与工程约束</span></a></h2><p>LLM存在三个核心限制：</p><ol><li><strong>幻觉（Hallucination）</strong>：模型会自信地生成符合统计规律但事实错误的内容。这是有损压缩的必然代价。</li><li><strong>知识截止日期</strong>：训练数据存在时间边界，模型无法感知训练后发生的事件。</li><li><strong>计算成本</strong>：每次推理需要在万亿参数中执行矩阵乘法，延迟和费用都远高于传统API。</li></ol><p>工程应对策略：</p><table><thead><tr><th>问题</th><th>解决方案</th></tr></thead><tbody><tr><td>幻觉</td><td>通过RAG动态注入事实信息，或使用温度=0降低随机性</td></tr><tr><td>知识过时</td><td>集成实时检索系统，或定期更新知识库</td></tr><tr><td>计算成本</td><td>缓存常见请求，使用较小模型处理简单任务</td></tr></tbody></table><h2 id="架构定位" tabindex="-1"><a class="header-anchor" href="#架构定位"><span>架构定位</span></a></h2><p>在AI工程栈中，LLM扮演&quot;推理引擎&quot;角色，负责理解意图、生成文本、做出决策。但它需要与其他组件配合：</p><ul><li><strong>RAG（检索增强生成）</strong>：为LLM提供事实依据，降低幻觉风险</li><li><strong>Agent（智能体）</strong>：调用LLM进行规划，并执行具体操作</li><li><strong>MCP（模型上下文协议）</strong>：标准化LLM与外部工具的交互接口</li></ul><p>将LLM视为分布式系统中的一个微服务：它处理自然语言理解的核心逻辑，但数据持久化、任务编排、安全控制仍需传统工程手段实现。</p>',9))])}const c=t(p,[["render",o]]),u=JSON.parse('{"path":"/guide/01-concepts/llm.html","title":"LLM：概率推理引擎","lang":"zh-CN","frontmatter":{"title":"LLM：概率推理引擎","order":1,"description":"LLM：概率推理引擎 从确定性到概率性的跨越 传统软件工程追求精确：相同的输入必然产生相同的输出。但现实世界的许多任务本质上是模糊的——用户意图含糊不清、业务规则存在灰色地带、边界条件难以穷举。如果用rule-based代码处理这些模糊场景，你会陷入无穷无尽的if-else分支，最终发现这条路根本不可行。 LLM的价值恰恰在此：它基于概率统计训练，天然...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM：概率推理引擎\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2026-01-09T11:15:14.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Alden\\",\\"url\\":\\"https://aldenwangexis.github.io/\\"}]}"],["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/llm-docs/guide/01-concepts/llm.html"}],["meta",{"property":"og:site_name","content":"LLM工程实践教程"}],["meta",{"property":"og:title","content":"LLM：概率推理引擎"}],["meta",{"property":"og:description","content":"LLM：概率推理引擎 从确定性到概率性的跨越 传统软件工程追求精确：相同的输入必然产生相同的输出。但现实世界的许多任务本质上是模糊的——用户意图含糊不清、业务规则存在灰色地带、边界条件难以穷举。如果用rule-based代码处理这些模糊场景，你会陷入无穷无尽的if-else分支，最终发现这条路根本不可行。 LLM的价值恰恰在此：它基于概率统计训练，天然..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-01-09T11:15:14.000Z"}],["meta",{"property":"article:modified_time","content":"2026-01-09T11:15:14.000Z"}]]},"git":{"createdTime":1767872442000,"updatedTime":1767957314000,"contributors":[{"name":"AldenWangExis","username":"AldenWangExis","email":"wangzihao286@126.com","commits":6,"url":"https://github.com/AldenWangExis"}]},"readingTime":{"minutes":7.22,"words":2165},"filePathRelative":"guide/01-concepts/llm.md","autoDesc":true}');export{c as comp,u as data};
