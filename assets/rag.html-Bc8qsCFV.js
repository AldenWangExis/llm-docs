import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as s,f as n,d as o,r as a,o as i}from"./app-Cp5jA_bi.js";const p={};function l(d,t){const e=a("Mermaid");return i(),s("div",null,[t[0]||(t[0]=n('<h1 id="rag-检索增强生成" tabindex="-1"><a class="header-anchor" href="#rag-检索增强生成"><span>RAG：检索增强生成</span></a></h1><h2 id="问题场景-llm的知识盲区" tabindex="-1"><a class="header-anchor" href="#问题场景-llm的知识盲区"><span>问题场景：LLM的知识盲区</span></a></h2><p>LLM面临两个结构性缺陷：</p><ol><li><strong>知识截止日期</strong>：训练数据存在时间边界。模型无法感知最新的API文档、今天的股价、昨天的新闻。</li><li><strong>私有数据盲点</strong>：模型从未见过企业内部的代码库、设计文档、运维日志。</li></ol><p>这两个问题导致实际应用中频繁出现&quot;一本正经地胡说八道&quot;（幻觉）。让模型解释公司内部的微服务架构，它会基于统计规律虚构出看似合理的答案。</p><p><strong>RAG（Retrieval-Augmented Generation）</strong> 通过在推理前动态注入外部知识，将&quot;闭卷考试&quot;转换为&quot;开卷考试&quot;。</p><h2 id="架构类比-mvc中的数据持久层" tabindex="-1"><a class="header-anchor" href="#架构类比-mvc中的数据持久层"><span>架构类比：MVC中的数据持久层</span></a></h2><p>对于熟悉Web开发的工程师，RAG的工作流可以映射到经典的MVC模式：</p>',8)),o(e,{code:"eJxLL0osyFDwCeJSAILQ4tSi6OdTVjzr2P5i/fZnG5tiFXR17RSc8/NKivJzcoCSCKaVwrP5S1+sX/Ri+/rnUzbGgvUjZMH6wlKTS/KLXJyiNSAsBRcnoK6pG571rnvW0/hkZ+vTjU2aEJ0wpWB9vvkpqTnRYNJKwcfH91nfiucT2p7umfpsch9EOVgOYkdmank0iACavGDP0z39T3umvdg3+Wn7LohKMFFcUpmTirAjLTMnx0o51TDNNC0NSR5iKEQyLS3NJNWQCwCFYHFW"}),t[1]||(t[1]=n('<p><strong>传统MVC流程</strong>：</p><ol><li>Controller接收HTTP请求</li><li>查询MySQL获取数据</li><li>Model处理业务逻辑</li><li>View渲染模板并返回HTML</li></ol><p><strong>RAG流程</strong>：</p><ol><li>解析用户问题（Query）</li><li>在向量数据库中检索相关文档</li><li>将文档与问题拼接，发送给LLM</li><li>LLM基于检索内容生成答案</li></ol><p>关键区别在于：传统数据库执行精确匹配（WHERE id=123），而向量数据库执行语义相似度搜索（寻找在高维空间中距离最近的向量）。</p><h2 id="核心流水线-从文档到答案" tabindex="-1"><a class="header-anchor" href="#核心流水线-从文档到答案"><span>核心流水线：从文档到答案</span></a></h2><p>RAG系统的生命周期分为两个阶段：<strong>索引构建（Indexing）</strong> 和 <strong>检索生成（Retrieval &amp; Generation）</strong>。</p><h3 id="阶段一-索引构建" tabindex="-1"><a class="header-anchor" href="#阶段一-索引构建"><span>阶段一：索引构建</span></a></h3><p>将原始文档转换为可检索的向量表示。</p>',9)),o(e,{code:"eJxLL0osyFDwCeJSAAKX/OTi6Kd9858u7342rf3ZwsWxCrq6dgrOGaV52Zl56dEQwacd7c8722PBOmBSYHWuuUmpKSkghU8nTHzZ3v+0ZxpEFVwCrCwsNbkkv8jFKVoDouzZ1A3Petc93TVZE6IaTBSXVOakIoxPy8zJsVJOS0s1TDVEkkcYDFGQaghSgqQAZhdcHqSCCwA87Vw7"}),t[2]||(t[2]=n('<p><strong>步骤1：文档切片（Chunking）</strong></p><p>LLM的上下文窗口有限，且长文本会导致语义模糊。需要将文档拆分为独立的语义单元。</p><p>策略权衡：</p><ul><li><strong>固定长度切片</strong>：实现简单，但可能在句子中间截断，破坏语义完整性</li><li><strong>语义边界切片</strong>：按段落、章节标题切分，保留结构信息，但块大小不均匀</li><li><strong>滑动窗口</strong>：相邻片段保留重叠区域（如50 Token），避免关键信息断裂，但增加存储成本</li></ul><p>核心权衡：<strong>语义完整性 vs 检索粒度 vs 存储成本</strong></p><p><strong>步骤2：向量化（Embedding）</strong></p><p>调用专门的Embedding模型将文本转换为高维向量（通常1536维或更高）。这些向量捕获语义特征，使得&quot;语义相似&quot;的文本在向量空间中距离较近。</p><p>设计要点：索引阶段与检索阶段必须使用同一个Embedding模型，否则向量空间不一致，检索失效。</p><p>这些向量捕获了文本的语义特征。&quot;语义相似&quot;的文本在向量空间中距离较近，即使它们使用不同的词汇。</p><p><strong>步骤3：存储到向量数据库</strong></p><p>将向量及其关联的元数据（原始文本、来源、时间戳）存入向量数据库。常用系统包括：</p><table><thead><tr><th>数据库</th><th>特点</th><th>适用场景</th></tr></thead><tbody><tr><td>Pinecone</td><td>全托管，低延迟</td><td>生产环境快速部署</td></tr><tr><td>Milvus</td><td>开源，高性能</td><td>大规模私有化部署</td></tr><tr><td>pgvector</td><td>PostgreSQL扩展</td><td>与现有PG栈集成</td></tr><tr><td>Chroma</td><td>轻量级，内存型</td><td>原型开发与测试</td></tr></tbody></table><h3 id="阶段二-检索与生成" tabindex="-1"><a class="header-anchor" href="#阶段二-检索与生成"><span>阶段二：检索与生成</span></a></h3><p>用户提问时的实时流程。</p>',14)),o(e,{code:"eJxt0E1Kw0AYBuB9TzEXyAW6KFQKLqxF/NuHMEgWaWISF+4qiImoJKWxCi0piUWzaSyoKEnAy3S+SW5hfghMiVlkMe8z870zGj6/wEMB90T+TOWlFso/hVd1URAVfqijEw2riNcQdQIwvxvx0aWmY6kAh91d+hHTeNEwp1jQZbW3UyhijzPDgsc1PIQkmjRsv79fsuVb+r5KgxEEHnHvSlb+ijpcp1ONbSOw7E20zJ7CzH8u8ypgBFnfVHE1mdxPt11dro26gwGd/WyShESvYM/pp1/KGnDMoemvQ2busaxwezA1wHuhtwaEX/83APeaxBHxXZJEB6osKfq2y6+c17TG2eiKifNVdiJ1FmDadOWAZ7Lbc1I8SV2JxhNw560/K4jKig=="}),t[3]||(t[3]=n('<p><strong>步骤1：查询向量化</strong></p><p>用户的问题转换为向量，使用与索引阶段相同的Embedding模型。模型一致性是检索准确性的前提。</p><p><strong>步骤2：相似度检索</strong></p><p>在向量数据库中执行ANN（Approximate Nearest Neighbor）搜索，找到Top-K个最相似的文档片段。</p><p>关键参数：</p><ul><li><code>top_k</code>：返回结果数量。过少可能遗漏信息，过多会引入噪声并增加Token成本</li><li><code>threshold</code>：相似度阈值。过低会返回无关内容，过高可能无结果</li></ul><p><strong>步骤3：Prompt增强</strong></p><p>将检索到的文档拼接到用户问题中。Prompt结构设计需要平衡三个目标：</p><ol><li>明确指示模型基于资料回答（降低幻觉）</li><li>要求标注来源（提高可追溯性）</li><li>允许模型承认信息不足（避免强行生成）</li></ol><p><strong>步骤4：LLM生成答案</strong></p><p>将增强后的Prompt发送给LLM。此时模型的上下文窗口包含：系统指令 + 检索资料 + 用户问题，三者共同约束生成结果。</p><div class="hint-container tip"><p class="hint-container-title">工程价值</p><p>RAG将&quot;知识更新&quot;与&quot;模型训练&quot;解耦。更新知识库只需重新索引文档，无需重新训练模型。这使得知识迭代的成本从千万美元降至千美元级别。</p></div><h2 id="从朴素到工程化-rag架构演进" tabindex="-1"><a class="header-anchor" href="#从朴素到工程化-rag架构演进"><span>从朴素到工程化：RAG架构演进</span></a></h2><h3 id="朴素rag的局限" tabindex="-1"><a class="header-anchor" href="#朴素rag的局限"><span>朴素RAG的局限</span></a></h3><p>初代RAG系统采用线性流水线：切片 → 向量化 → 检索 → 生成。这种架构存在三个痛点：</p><ol><li><strong>召回率低</strong>：仅依赖语义相似度，可能遗漏相关文档</li><li><strong>噪声干扰</strong>：Top-K结果中可能包含无关内容</li><li><strong>上下文冗余</strong>：长文档片段占用大量Token，但有效信息稀疏</li></ol><h3 id="模块化rag的工程优化" tabindex="-1"><a class="header-anchor" href="#模块化rag的工程优化"><span>模块化RAG的工程优化</span></a></h3><p>生产级RAG系统引入多个优化模块：</p>',18)),o(e,{code:"eJxLL0osyFAIceFSAILA0tSiyujnU1Y869j+bP7SF+sXxSro6topBKWWF2WWpEZDxF629z5tmxkL1gGVgajKLy1JLYp+sX398ykbn7Ztfr52GlQRWAKsxqMyqSgzJfrZ9u1PJ3Q8W9zwfMsiiBqIBNS2osS87GigNc/6Jj3d1Q+zCSQKVuCcn1tQlFpcHP1kR9eTHd3PprU/7et+vmclRCFMFqzUPTUPqA/odB8f3+dT5j/rmABRBCaKSypzUuFeSMvMybFSTktLNUw1RJaGOB4im2oIkkfRDHYVTBYkzwUA2PKFtw=="}),t[4]||(t[4]=n(`<p><strong>查询重写（Query Rewriting）</strong></p><p>将用户的口语化问题转换为更适合检索的形式。例如：&quot;这个bug咋修？&quot; → &quot;如何定位并修复该问题？请提供调试步骤和可能的解决方案。&quot;</p><p>设计权衡：重写可以提高检索精度，但增加了一次LLM调用的延迟和成本。适用于用户输入质量不稳定的场景。</p><p><strong>混合检索（Hybrid Retrieval）</strong></p><p>结合向量检索（语义相似度）和关键词检索（BM25算法），提高召回率。</p><p>原理：向量检索擅长捕获语义，但对专有名词敏感度低；关键词检索精确匹配术语，但无法理解同义词。两者互补可以覆盖更多相关文档。</p><p><strong>重排序（Re-ranking）</strong></p><p>使用专门的Cross-Encoder模型对检索结果重新打分。与Embedding模型不同，Re-ranker直接计算query与每个文档的匹配度，精度更高但计算成本也更高。</p><p>架构决策：先用快速的向量检索筛选候选集（如Top-50），再用慢速的Re-ranker精排（如Top-3）。这种两阶段架构平衡了精度与性能。</p><p><strong>上下文压缩（Context Compression）</strong></p><p>在保留关键信息的前提下，压缩文档长度，节省Token成本。</p><p>策略：提取与问题最相关的句子，或使用小模型生成摘要。核心权衡是<strong>信息完整性 vs Token成本</strong>。</p><h2 id="特殊架构-缓存增强生成-cag" tabindex="-1"><a class="header-anchor" href="#特殊架构-缓存增强生成-cag"><span>特殊架构：缓存增强生成（CAG）</span></a></h2><p>对于知识相对静态的场景（如产品FAQ、API文档），可以采用CAG模式：</p><ol><li>预先将所有知识加载到LLM的KV缓存中</li><li>运行时直接推理，无需检索</li></ol><p><strong>优势</strong>：</p><ul><li>消除检索延迟</li><li>保证知识的完整性和一致性</li></ul><p><strong>限制</strong>：</p><ul><li>只适用于知识量小于上下文窗口的场景</li><li>无法动态更新知识</li></ul><h2 id="生产环境的工程挑战" tabindex="-1"><a class="header-anchor" href="#生产环境的工程挑战"><span>生产环境的工程挑战</span></a></h2><h3 id="数据质量问题" tabindex="-1"><a class="header-anchor" href="#数据质量问题"><span>数据质量问题</span></a></h3><p><strong>问题</strong>：文档包含大量HTML标签、格式噪声、重复内容<br><strong>解决思路</strong>：构建文档清洗流水线，标准化文本格式（移除标签、规范化空白字符、去重）</p><p>设计原则：清洗规则需要针对数据源特点定制。过度清洗可能丢失结构信息（如代码缩进），清洗不足则影响检索质量。</p><h3 id="检索精度优化" tabindex="-1"><a class="header-anchor" href="#检索精度优化"><span>检索精度优化</span></a></h3><p><strong>问题</strong>：用户问&quot;如何配置Redis集群&quot;，却检索到&quot;MongoDB集群配置&quot;<br><strong>解决思路</strong>：引入元数据过滤，限制检索范围</p><p>架构设计：在向量化时同步存储元数据（类别、版本、时间戳），检索时先过滤元数据再计算向量相似度。这种混合策略比纯向量检索更精准。</p><h3 id="成本控制" tabindex="-1"><a class="header-anchor" href="#成本控制"><span>成本控制</span></a></h3><p><strong>问题</strong>：每次请求检索大量文档，Token成本激增<br><strong>解决思路</strong>：实现智能路由，简单问题直接用LLM回答，复杂问题才启动RAG</p><p>决策逻辑：通过问题分类器判断是否需要外部知识。这种分层架构将成本集中在真正需要RAG的场景。</p><h2 id="架构定位" tabindex="-1"><a class="header-anchor" href="#架构定位"><span>架构定位</span></a></h2><p>RAG在AI工程栈中扮演&quot;持久化层&quot;角色，解决LLM的知识边界问题。它与其他组件的关系：</p><ul><li><strong>LLM</strong>：RAG为其提供事实依据，降低幻觉风险</li><li><strong>Agent</strong>：Agent可以将RAG作为一个工具调用，按需检索知识</li><li><strong>MCP</strong>：通过MCP协议，Agent可以连接到多个RAG数据源</li></ul><p>典型架构模式：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>用户查询 → Agent决策 → [是否需要RAG?] </span></span>
<span class="line"><span>           ↓ 是</span></span>
<span class="line"><span>         RAG检索 → 注入上下文 → LLM生成 → 返回结果</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>将RAG视为分布式系统中的缓存层：它在LLM与原始数据之间提供了一个高效的索引和检索机制，使得AI应用能够快速访问海量的外部知识。</p>`,35))])}const c=r(p,[["render",l]]),u=JSON.parse('{"path":"/guide/01-concepts/rag.html","title":"RAG：检索增强生成","lang":"zh-CN","frontmatter":{"title":"RAG：检索增强生成","order":2,"description":"RAG：检索增强生成 问题场景：LLM的知识盲区 LLM面临两个结构性缺陷： 知识截止日期：训练数据存在时间边界。模型无法感知最新的API文档、今天的股价、昨天的新闻。 私有数据盲点：模型从未见过企业内部的代码库、设计文档、运维日志。 这两个问题导致实际应用中频繁出现\\"一本正经地胡说八道\\"（幻觉）。让模型解释公司内部的微服务架构，它会基于统计规律虚构出...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"RAG：检索增强生成\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2026-01-08T11:53:16.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Alden\\",\\"url\\":\\"https://aldenwangexis.github.io/\\"}]}"],["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/llm-docs/guide/01-concepts/rag.html"}],["meta",{"property":"og:site_name","content":"LLM工程实践教程"}],["meta",{"property":"og:title","content":"RAG：检索增强生成"}],["meta",{"property":"og:description","content":"RAG：检索增强生成 问题场景：LLM的知识盲区 LLM面临两个结构性缺陷： 知识截止日期：训练数据存在时间边界。模型无法感知最新的API文档、今天的股价、昨天的新闻。 私有数据盲点：模型从未见过企业内部的代码库、设计文档、运维日志。 这两个问题导致实际应用中频繁出现\\"一本正经地胡说八道\\"（幻觉）。让模型解释公司内部的微服务架构，它会基于统计规律虚构出..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-01-08T11:53:16.000Z"}],["meta",{"property":"article:modified_time","content":"2026-01-08T11:53:16.000Z"}]]},"git":{"createdTime":1767872442000,"updatedTime":1767873196000,"contributors":[{"name":"AldenWangExis","username":"AldenWangExis","email":"wangzihao286@126.com","commits":3,"url":"https://github.com/AldenWangExis"}]},"readingTime":{"minutes":8.31,"words":2492},"filePathRelative":"guide/01-concepts/rag.md","autoDesc":true}');export{c as comp,u as data};
