import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,f as s,d as n,r as l,o as h}from"./app-BEzUu1Zf.js";const k={};function p(r,i){const a=l("Mermaid");return h(),e("div",null,[i[0]||(i[0]=s('<h1 id="rag-检索增强生成" tabindex="-1"><a class="header-anchor" href="#rag-检索增强生成"><span>RAG：检索增强生成</span></a></h1><h2 id="问题场景-llm的知识盲区" tabindex="-1"><a class="header-anchor" href="#问题场景-llm的知识盲区"><span>问题场景：LLM的知识盲区</span></a></h2><p>LLM面临两个结构性缺陷：</p><ol><li><strong>知识截止日期</strong>：训练数据存在时间边界。模型无法感知最新的API文档、今天的股价、昨天的新闻。</li><li><strong>私有数据盲点</strong>：模型从未见过企业内部的代码库、设计文档、运维日志。</li></ol><p>这两个问题导致实际应用中频繁出现&quot;一本正经地胡说八道&quot;（幻觉）。让模型解释公司内部的微服务架构，它会基于统计规律虚构出看似合理的答案。</p><p><strong>RAG（Retrieval-Augmented Generation）</strong> 通过在推理前动态注入外部知识，将&quot;闭卷考试&quot;转换为&quot;开卷考试&quot;。</p><h2 id="架构类比-mvc中的数据持久层" tabindex="-1"><a class="header-anchor" href="#架构类比-mvc中的数据持久层"><span>架构类比：MVC中的数据持久层</span></a></h2><p>对于熟悉Web开发的工程师，RAG的工作流可以映射到经典的MVC模式：</p>',8)),n(a,{code:"eJxLL0osyFDwCeJSAILQ4tSi6OdTVjzr2P5i/fZnG5tiFXR17RSc8/NKivJzcoCSCKaVwrP5S1+sX/Ri+/rnUzbGgvUjZMH6wlKTS/KLXJyiNSAsBRcnoK6pG571rnvW0/hkZ+vTjU2aEJ0wpWB9vvkpqTnRYNJKwcfH91nfiucT2p7umfpsch9EOVgOYkdmank0iACavGDP0z39T3umvdg3+Wn7LohKMFFcUpmTirAjLTMnx0o51TDNNC0NSR5iKEQyLS3NJNWQCwCFYHFW"}),i[1]||(i[1]=s('<p><strong>传统MVC流程</strong>：</p><ol><li>Controller接收HTTP请求</li><li>查询MySQL获取数据</li><li>Model处理业务逻辑</li><li>View渲染模板并返回HTML</li></ol><p><strong>RAG流程</strong>：</p><ol><li>解析用户问题（Query）</li><li>在向量数据库中检索相关文档</li><li>将文档与问题拼接，发送给LLM</li><li>LLM基于检索内容生成答案</li></ol><p>关键区别在于：传统数据库执行精确匹配（WHERE id=123），而向量数据库执行语义相似度搜索（寻找在高维空间中距离最近的向量）。</p><h2 id="核心流水线-从文档到答案" tabindex="-1"><a class="header-anchor" href="#核心流水线-从文档到答案"><span>核心流水线：从文档到答案</span></a></h2><p>RAG系统的生命周期分为两个阶段：<strong>索引构建（Indexing）</strong> 和 <strong>检索生成（Retrieval &amp; Generation）</strong>。</p><h3 id="阶段一-索引构建" tabindex="-1"><a class="header-anchor" href="#阶段一-索引构建"><span>阶段一：索引构建</span></a></h3><p>将原始文档转换为可检索的向量表示。</p>',9)),n(a,{code:"eJxLL0osyFDwCeJSAAKX/OTi6Kd9858u7342rf3ZwsWxCrq6dgrOGaV52Zl56dEQwacd7c8722PBOmBSYHWuuUmpKSkghU8nTHzZ3v+0ZxpEFVwCrCwsNbkkv8jFKVoDouzZ1A3Petc93TVZE6IaTBSXVOakIoxPy8zJsVJOS0s1TDVEkkcYDFGQaghSgqQAZhdcHqSCCwA87Vw7"}),i[2]||(i[2]=s(`<p><strong>步骤1：文档切片（Chunking）</strong></p><p>LLM的上下文窗口有限，且长文本会导致语义模糊。需要将文档拆分为独立的语义单元。</p><p>常见策略：</p><ul><li><strong>固定长度切片</strong>：每512个Token切一次，简单但可能破坏语义完整性</li><li><strong>语义边界切片</strong>：按段落、章节标题切分，保留结构信息</li><li><strong>滑动窗口</strong>：相邻片段保留重叠区域，避免信息断裂</li></ul><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 伪代码示例</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> chunk_document</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">doc</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> str</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> size</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> int</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 512</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> overlap</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> int</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 50</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    tokens </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> tokenize</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(doc)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    chunks </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> []</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> i </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> range</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(tokens), size </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> overlap):</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        chunk </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokens[i:i</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">+</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">size]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        chunks.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">append</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(chunk)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> chunks</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>步骤2：向量化（Embedding）</strong></p><p>调用专门的Embedding模型（如OpenAI的<code>text-embedding-3</code>或开源的<code>bge-large</code>），将文本转换为高维向量。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># API调用示例</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">chunk_text </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;RAG通过检索外部知识增强LLM的生成能力&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">vector </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> embedding_model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">encode</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(chunk_text)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 输出: [0.023, -0.145, 0.891, ..., 0.334]  # 1536维向量</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这些向量捕获了文本的语义特征。&quot;语义相似&quot;的文本在向量空间中距离较近，即使它们使用不同的词汇。</p><p><strong>步骤3：存储到向量数据库</strong></p><p>将向量及其关联的元数据（原始文本、来源、时间戳）存入向量数据库。常用系统包括：</p><table><thead><tr><th>数据库</th><th>特点</th><th>适用场景</th></tr></thead><tbody><tr><td>Pinecone</td><td>全托管，低延迟</td><td>生产环境快速部署</td></tr><tr><td>Milvus</td><td>开源，高性能</td><td>大规模私有化部署</td></tr><tr><td>pgvector</td><td>PostgreSQL扩展</td><td>与现有PG栈集成</td></tr><tr><td>Chroma</td><td>轻量级，内存型</td><td>原型开发与测试</td></tr></tbody></table><h3 id="阶段二-检索与生成" tabindex="-1"><a class="header-anchor" href="#阶段二-检索与生成"><span>阶段二：检索与生成</span></a></h3><p>用户提问时的实时流程。</p>`,14)),n(a,{code:"eJxt0E1Kw0AYBuB9TzEXyAW6KFQKLqxF/NuHMEgWaWISF+4qiImoJKWxCi0piUWzaSyoKEnAy3S+SW5hfghMiVlkMe8z870zGj6/wEMB90T+TOWlFso/hVd1URAVfqijEw2riNcQdQIwvxvx0aWmY6kAh91d+hHTeNEwp1jQZbW3UyhijzPDgsc1PIQkmjRsv79fsuVb+r5KgxEEHnHvSlb+ijpcp1ONbSOw7E20zJ7CzH8u8ypgBFnfVHE1mdxPt11dro26gwGd/WyShESvYM/pp1/KGnDMoemvQ2busaxwezA1wHuhtwaEX/83APeaxBHxXZJEB6osKfq2y6+c17TG2eiKifNVdiJ1FmDadOWAZ7Lbc1I8SV2JxhNw560/K4jKig=="}),i[3]||(i[3]=s(`<p><strong>步骤1：查询向量化</strong></p><p>用户的问题同样需要转换为向量，使用与索引阶段相同的Embedding模型。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">query </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;如何优化RAG系统的检索精度？&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">query_vector </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> embedding_model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">encode</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(query)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>步骤2：相似度检索</strong></p><p>在向量数据库中执行ANN（Approximate Nearest Neighbor）搜索，找到Top-K个最相似的文档片段。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">results </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> vector_db.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">search</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    vector</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">query_vector,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    top_k</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 返回最相关的3个片段</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    threshold</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.7</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  # 相似度阈值</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>步骤3：Prompt增强</strong></p><p>将检索到的文档拼接到用户问题中，构建增强Prompt。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">augmented_prompt </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">基于以下参考资料回答问题：</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">【资料1】</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">results[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">].text</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">【资料2】</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">results[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">].text</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">【资料3】</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">results[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">].text</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">用户问题：</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">query</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">要求：</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">1. 答案必须基于上述资料</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">2. 如果资料不足以回答，明确说明</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">3. 引用资料时标注来源编号</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&quot;&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>步骤4：LLM生成答案</strong></p><p>将增强后的Prompt发送给LLM，模型基于注入的事实信息生成答案。</p><div class="hint-container tip"><p class="hint-container-title">工程价值</p><p>RAG将&quot;知识更新&quot;与&quot;模型训练&quot;解耦。更新知识库只需重新索引文档，无需重新训练模型。这使得知识迭代的成本从千万美元降至千美元级别。</p></div><h2 id="从朴素到工程化-rag架构演进" tabindex="-1"><a class="header-anchor" href="#从朴素到工程化-rag架构演进"><span>从朴素到工程化：RAG架构演进</span></a></h2><h3 id="朴素rag的局限" tabindex="-1"><a class="header-anchor" href="#朴素rag的局限"><span>朴素RAG的局限</span></a></h3><p>初代RAG系统采用线性流水线：切片 → 向量化 → 检索 → 生成。这种架构存在三个痛点：</p><ol><li><strong>召回率低</strong>：仅依赖语义相似度，可能遗漏相关文档</li><li><strong>噪声干扰</strong>：Top-K结果中可能包含无关内容</li><li><strong>上下文冗余</strong>：长文档片段占用大量Token，但有效信息稀疏</li></ol><h3 id="模块化rag的工程优化" tabindex="-1"><a class="header-anchor" href="#模块化rag的工程优化"><span>模块化RAG的工程优化</span></a></h3><p>生产级RAG系统引入多个优化模块：</p>`,18)),n(a,{code:"eJxLL0osyFAIceFSAILA0tSiyujnU1Y869j+bP7SF+sXxSro6topBKWWF2WWpEZDxF629z5tmxkL1gGVgajKLy1JLYp+sX398ykbn7Ztfr52GlQRWAKsxqMyqSgzJfrZ9u1PJ3Q8W9zwfMsiiBqIBNS2osS87GigNc/6Jj3d1Q+zCSQKVuCcn1tQlFpcHP1kR9eTHd3PprU/7et+vmclRCFMFqzUPTUPqA/odB8f3+dT5j/rmABRBCaKSypzUuFeSMvMybFSTktLNUw1RJaGOB4im2oIkkfRDHYVTBYkzwUA2PKFtw=="}),i[4]||(i[4]=s(`<p><strong>查询重写（Query Rewriting）</strong></p><p>将用户的口语化问题转换为更适合检索的形式。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>原始: &quot;这个bug咋修？&quot;</span></span>
<span class="line"><span>重写: &quot;如何定位并修复该问题？请提供调试步骤和可能的解决方案。&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>混合检索（Hybrid Retrieval）</strong></p><p>结合向量检索（语义相似度）和关键词检索（BM25算法），提高召回率。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">vector_results </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> vector_search</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(query_vector, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">top_k</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">10</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">keyword_results </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> bm25_search</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(query_keywords, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">top_k</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">10</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">combined_results </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> merge_and_deduplicate</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(vector_results, keyword_results)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>重排序（Re-ranking）</strong></p><p>使用专门的排序模型对检索结果重新打分，将最相关的文档排在前面。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用Cross-Encoder模型计算query与每个文档的相关性分数</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">scores </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> reranker.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">predict</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">([(query, doc) </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> doc </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> combined_results])</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">final_results </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> sort_by_score</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(combined_results, scores)[:</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>上下文压缩（Context Compression）</strong></p><p>在保留关键信息的前提下，压缩文档长度，节省Token成本。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 提取与问题最相关的句子</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">compressed </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> extract_relevant_sentences</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    document</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">final_results[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">],</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    query</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">query,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    max_sentences</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">5</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="特殊架构-缓存增强生成-cag" tabindex="-1"><a class="header-anchor" href="#特殊架构-缓存增强生成-cag"><span>特殊架构：缓存增强生成（CAG）</span></a></h2><p>对于知识相对静态的场景（如产品FAQ、API文档），可以采用CAG模式：</p><ol><li>预先将所有知识加载到LLM的KV缓存中</li><li>运行时直接推理，无需检索</li></ol><p><strong>优势</strong>：</p><ul><li>消除检索延迟</li><li>保证知识的完整性和一致性</li></ul><p><strong>限制</strong>：</p><ul><li>只适用于知识量小于上下文窗口的场景</li><li>无法动态更新知识</li></ul><h2 id="生产环境的工程挑战" tabindex="-1"><a class="header-anchor" href="#生产环境的工程挑战"><span>生产环境的工程挑战</span></a></h2><h3 id="数据质量问题" tabindex="-1"><a class="header-anchor" href="#数据质量问题"><span>数据质量问题</span></a></h3><p><strong>问题</strong>：文档包含大量HTML标签、格式噪声、重复内容<br><strong>解决</strong>：构建文档清洗流水线，标准化文本格式</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> clean_document</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">raw_text</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> str</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) -&gt; </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">str</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 移除HTML标签</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    text </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> strip_html</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(raw_text)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 规范化空白字符</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    text </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> normalize_whitespace</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(text)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 移除页眉页脚</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    text </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> remove_headers_footers</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(text)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> text</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="检索精度优化" tabindex="-1"><a class="header-anchor" href="#检索精度优化"><span>检索精度优化</span></a></h3><p><strong>问题</strong>：用户问&quot;如何配置Redis集群&quot;，却检索到&quot;MongoDB集群配置&quot;<br><strong>解决</strong>：引入元数据过滤，限制检索范围</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">results </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> vector_db.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">search</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    vector</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">query_vector,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    filter</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;category&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;redis&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;version&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&gt;=7.0&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    top_k</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="成本控制" tabindex="-1"><a class="header-anchor" href="#成本控制"><span>成本控制</span></a></h3><p><strong>问题</strong>：每次请求检索大量文档，Token成本激增<br><strong>解决</strong>：实现智能路由，简单问题直接用LLM回答，复杂问题才启动RAG</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> is_simple_question</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(query):</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> llm.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">predict</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(query)  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 不调用RAG</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">else</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> rag_pipeline.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">run</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(query)  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 完整RAG流程</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="架构定位" tabindex="-1"><a class="header-anchor" href="#架构定位"><span>架构定位</span></a></h2><p>RAG在AI工程栈中扮演&quot;持久化层&quot;角色，解决LLM的知识边界问题。它与其他组件的关系：</p><ul><li><strong>LLM</strong>：RAG为其提供事实依据，降低幻觉风险</li><li><strong>Agent</strong>：Agent可以将RAG作为一个工具调用，按需检索知识</li><li><strong>MCP</strong>：通过MCP协议，Agent可以连接到多个RAG数据源</li></ul><p>典型架构模式：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>用户查询 → Agent决策 → [是否需要RAG?] </span></span>
<span class="line"><span>           ↓ 是</span></span>
<span class="line"><span>         RAG检索 → 注入上下文 → LLM生成 → 返回结果</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>将RAG视为分布式系统中的缓存层：它在LLM与原始数据之间提供了一个高效的索引和检索机制，使得AI应用能够快速访问海量的外部知识。</p>`,35))])}const o=t(k,[["render",p]]),A=JSON.parse('{"path":"/guide/01-concepts/rag.html","title":"RAG：检索增强生成","lang":"zh-CN","frontmatter":{"title":"RAG：检索增强生成","icon":"database","order":2,"description":"RAG：检索增强生成 问题场景：LLM的知识盲区 LLM面临两个结构性缺陷： 知识截止日期：训练数据存在时间边界。模型无法感知最新的API文档、今天的股价、昨天的新闻。 私有数据盲点：模型从未见过企业内部的代码库、设计文档、运维日志。 这两个问题导致实际应用中频繁出现\\"一本正经地胡说八道\\"（幻觉）。让模型解释公司内部的微服务架构，它会基于统计规律虚构出...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"RAG：检索增强生成\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2026-01-08T11:40:42.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Alden\\",\\"url\\":\\"https://aldenwangexis.github.io/\\"}]}"],["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/llm-docs/guide/01-concepts/rag.html"}],["meta",{"property":"og:site_name","content":"LLM工程实践教程"}],["meta",{"property":"og:title","content":"RAG：检索增强生成"}],["meta",{"property":"og:description","content":"RAG：检索增强生成 问题场景：LLM的知识盲区 LLM面临两个结构性缺陷： 知识截止日期：训练数据存在时间边界。模型无法感知最新的API文档、今天的股价、昨天的新闻。 私有数据盲点：模型从未见过企业内部的代码库、设计文档、运维日志。 这两个问题导致实际应用中频繁出现\\"一本正经地胡说八道\\"（幻觉）。让模型解释公司内部的微服务架构，它会基于统计规律虚构出..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-01-08T11:40:42.000Z"}],["meta",{"property":"article:modified_time","content":"2026-01-08T11:40:42.000Z"}]]},"git":{"createdTime":1767872442000,"updatedTime":1767872442000,"contributors":[{"name":"AldenWangExis","username":"AldenWangExis","email":"wangzihao286@126.com","commits":1,"url":"https://github.com/AldenWangExis"}]},"readingTime":{"minutes":7.4,"words":2220},"filePathRelative":"guide/01-concepts/rag.md","autoDesc":true}');export{o as comp,A as data};
